{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"wYkQDRe7TfPL"},"outputs":[],"source":["# DO NOT CHANGE ANYTHING IN THIS CELL\n","import numpy as np\n","import pandas as pd\n","import os\n","import matplotlib.pyplot as plt\n","import cv2\n","import tensorflow as tf\n","import tensorflow.keras\n","import time\n","import pytz\n","import imutils\n","import hashlib\n","import pickle\n","from datetime import datetime\n","\n","# scikit learn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix\n","\n","# tensorflow libraries\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, AveragePooling2D, Dense, Activation, Dropout, Flatten, Input\n","from tensorflow.keras.metrics import categorical_accuracy\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras.optimizers import *\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","from tensorflow.keras.applications import MobileNetV2\n","\n","from typing import List\n","from pathlib import Path\n","from imutils.video import VideoStream\n","from PIL import Image\n","\n","label_map = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wsV0ug-kXeTZ"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gZ6ioo6YTfPP","outputId":"fbbb7d78-6383-4054-adff-914e40eb6f12"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading from previously processed training data X and labels Y\n","Validated that X.shape is (32298, 48, 48, 1)\n","Validated that len(Y) is 32298\n"]}],"source":["\n","npy_data = 'dataset/numpy_images.npy'\n","label_data = 'dataset/labels.pkl'\n","if Path(npy_data).is_file() and Path(label_data).is_file():\n","    print('Loading from previously processed training data X and labels Y')\n","    with open(npy_data, 'rb') as f:\n","        X = np.load(f)\n","    with open(label_data, 'rb') as f:\n","        Y = pickle.load(f)\n","        labels = [label_map[e] for e in Y]\n","else:\n","    print('No training numpy array saved previously. Please run the preprocessing notebook.')\n","\n","\n","assert X.shape == (32298, 48, 48, 1)\n","print(f'Validated that X.shape is (32298, 48, 48, 1)')\n","\n","assert len(Y) == X.shape[0]\n","print(f'Validated that len(Y) is {len(Y)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"be4faef86c3c5635697f10939547edd5c8760308","id":"FyTAIuzjTfPX","outputId":"37d38460-c2b4-499b-dd67-83b1f0242288"},"outputs":[{"name":"stderr","output_type":"stream","text":["Using TensorFlow backend.\n"]},{"name":"stdout","output_type":"stream","text":["(32298,)\n","(32298, 7)\n"]}],"source":["# your code here\n","from keras.utils import to_categorical\n","print(Y.shape)\n","Y = to_categorical(Y)\n","print(Y.shape)\n","X_train, X_val, y_train, y_val = train_test_split(X, Y,test_size=0.1, random_state=1234)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_IzpDCZlTfPa"},"outputs":[],"source":["# your code here\n","def generate_id(digest_size=4):\n","    hash = hashlib.blake2b(digest_size=digest_size)\n","    hash.update(str(time.time()).encode('utf-8'))\n","    return hash.hexdigest()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"c8eaecce539d06c983ed73142ac1484dbfa5e970","id":"q0dM8F1iTfPc"},"outputs":[],"source":["def simple_convnet():\n","    model = Sequential([Conv2D(64, (8,8), padding='same', activation='relu', input_shape=(48,48,1)),\n","                      BatchNormalization(),\n","    MaxPooling2D(),\n","    Dropout(0.2),\n","    Conv2D(128, (8,8), padding='same', activation='relu'),\n","    BatchNormalization(),\n","    MaxPooling2D(),\n","    Dropout(0.2),\n","    Conv2D(256, (8,8), padding='same', activation='relu'),\n","    BatchNormalization(),\n","    MaxPooling2D(),\n","    Dropout(0.2),\n","    Flatten(),\n","    Dense(512, activation='relu'),\n","    Dense(7,activation = 'softmax')\n","  ])\n","    return model\n","\n","model = simple_convnet()\n","model.compile(optimizer='adam',\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f2Vc9pibTfPf","outputId":"b115222d-57b9-46a8-8692-7b7a11e6322a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_9 (Conv2D)            (None, 48, 48, 64)        4160      \n","_________________________________________________________________\n","batch_normalization_9 (Batch (None, 48, 48, 64)        256       \n","_________________________________________________________________\n","max_pooling2d_9 (MaxPooling2 (None, 24, 24, 64)        0         \n","_________________________________________________________________\n","dropout_9 (Dropout)          (None, 24, 24, 64)        0         \n","_________________________________________________________________\n","conv2d_10 (Conv2D)           (None, 24, 24, 128)       524416    \n","_________________________________________________________________\n","batch_normalization_10 (Batc (None, 24, 24, 128)       512       \n","_________________________________________________________________\n","max_pooling2d_10 (MaxPooling (None, 12, 12, 128)       0         \n","_________________________________________________________________\n","dropout_10 (Dropout)         (None, 12, 12, 128)       0         \n","_________________________________________________________________\n","conv2d_11 (Conv2D)           (None, 12, 12, 256)       2097408   \n","_________________________________________________________________\n","batch_normalization_11 (Batc (None, 12, 12, 256)       1024      \n","_________________________________________________________________\n","max_pooling2d_11 (MaxPooling (None, 6, 6, 256)         0         \n","_________________________________________________________________\n","dropout_11 (Dropout)         (None, 6, 6, 256)         0         \n","_________________________________________________________________\n","flatten_3 (Flatten)          (None, 9216)              0         \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 512)               4719104   \n","_________________________________________________________________\n","dense_7 (Dense)              (None, 7)                 3591      \n","=================================================================\n","Total params: 7,350,471\n","Trainable params: 7,349,575\n","Non-trainable params: 896\n","_________________________________________________________________\n"]}],"source":["# check the summary of your model (you don't have to do anything in this cell)\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"5004be413385dbdf6c3967d34c59e541095ea667","id":"wVjjE4kfTfPh"},"outputs":[],"source":["PATH ='/content/dataset'\n","train_dir = os.path.join(PATH, 'train')\n","validation_dir = os.path.join(PATH, 'validation')\n","batch_size = 128\n","epochs = 15\n","train_image_generator = ImageDataGenerator(\n","                    rotation_range=10,\n","                    width_shift_range=.15,\n","                    height_shift_range=.15,\n","                    #horizontal_flip=True,\n","                    zoom_range=0.1\n",")\n","validation_image_generator = ImageDataGenerator()\n","train_data_gen = train_image_generator.flow(X_train ,y_train,batch_size = batch_size)\n","\n","val_data_gen = validation_image_generator.flow(X_val ,y_val,batch_size = batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"5004be413385dbdf6c3967d34c59e541095ea667","id":"B_J29Z6sTfPk","scrolled":true},"outputs":[],"source":["# DO NOT CHANGE ANYTHING IN THIS CELL\n","utc_now = pytz.utc.localize(datetime.utcnow())\n","date_today = utc_now.astimezone(pytz.timezone('Asia/Shanghai')).strftime('%Y-%m-%d')\n","#path_model=f'/content/drive/My Drive/checkpoints/convnet/{date_today}/' + '{epoch:02d}-{val_loss:.6f}.hdf5'\n","#Path(f'/content/drive/My Drive/checkpoints/convnet/{date_today}').mkdir(parents=True, exist_ok=True)\n","path_model=f'checkpoints/convnet/{date_today}/' + '{epoch:02d}-{val_loss:.6f}.hdf5'\n","Path(f'checkpoints/convnet/{date_today}').mkdir(parents=True, exist_ok=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rbnqu2cwTfPm","outputId":"b75ac39a-9459-4102-e9fc-97b2cfea74c4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_9 (Conv2D)            (None, 48, 48, 64)        4160      \n","_________________________________________________________________\n","batch_normalization_9 (Batch (None, 48, 48, 64)        256       \n","_________________________________________________________________\n","max_pooling2d_9 (MaxPooling2 (None, 24, 24, 64)        0         \n","_________________________________________________________________\n","dropout_9 (Dropout)          (None, 24, 24, 64)        0         \n","_________________________________________________________________\n","conv2d_10 (Conv2D)           (None, 24, 24, 128)       524416    \n","_________________________________________________________________\n","batch_normalization_10 (Batc (None, 24, 24, 128)       512       \n","_________________________________________________________________\n","max_pooling2d_10 (MaxPooling (None, 12, 12, 128)       0         \n","_________________________________________________________________\n","dropout_10 (Dropout)         (None, 12, 12, 128)       0         \n","_________________________________________________________________\n","conv2d_11 (Conv2D)           (None, 12, 12, 256)       2097408   \n","_________________________________________________________________\n","batch_normalization_11 (Batc (None, 12, 12, 256)       1024      \n","_________________________________________________________________\n","max_pooling2d_11 (MaxPooling (None, 6, 6, 256)         0         \n","_________________________________________________________________\n","dropout_11 (Dropout)         (None, 6, 6, 256)         0         \n","_________________________________________________________________\n","flatten_3 (Flatten)          (None, 9216)              0         \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 512)               4719104   \n","_________________________________________________________________\n","dense_7 (Dense)              (None, 7)                 3591      \n","=================================================================\n","Total params: 7,350,471\n","Trainable params: 7,349,575\n","Non-trainable params: 896\n","_________________________________________________________________\n","Epoch 1/15\n","228/228 [==============================] - 826s 4s/step - loss: 2.3894 - accuracy: 0.2434 - val_loss: 2.1498 - val_accuracy: 0.2573 - lr: 0.0010\n","Epoch 2/15\n","102/228 [============>.................] - ETA: 8:35 - loss: 1.7731 - accuracy: 0.2690"]}],"source":["# implement training of the model\n","# your code here\n","model.summary()\n","history = model.fit(\n","    train_data_gen,\n","    epochs=15,\n","    batch_size= batch_size,\n","    validation_data=val_data_gen,\n","    callbacks=[\n","        ModelCheckpoint(filepath=path_model),\n","        EarlyStopping(patience=15),\n","        ReduceLROnPlateau(patience=6, factor=0.3)\n","    ]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zwE_RVrJNnxL"},"outputs":[],"source":["model.save('checkpoints/convne/checkpoint1.hdf5')\n","print(path_model)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python (utech_core)","language":"python","name":"utech_core"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":0}
